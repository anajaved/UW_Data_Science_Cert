{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "secret-apple",
   "metadata": {},
   "source": [
    "## Lesson 10: New Topic Identification\n",
    "### Author: Ana Javed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "color-shower",
   "metadata": {},
   "source": [
    "### Workplace Scenario\n",
    "\n",
    "Your next generation search engine startup was successful in having the ability to search for images based on their content. As a result, the startup received its second round of funding to be able to search news articles based on their topic. As the lead data scientist, you are tasked to build a model that classifies the topic of each article or newswire. \n",
    "\n",
    "For this assignment, you will leverage the RNN_KERAS.ipynb lab in the lesson. You are tasked to use the Keras Reuters newswire topics classification dataset. This dataset contains 11,228 newswires from Reuters, labeled with over 46 topics. Each wire is encoded as a sequence of word indexes. For convenience, words are indexed by overall frequency in the dataset, so that for instance the integer \"3\" encodes the 3rd most frequent word in the data. This allows for quick filtering operations such as: \"only consider the top 10,000 most common words, but eliminate the top 20 most common words\". As a convention, \"0\" does not stand for a specific word, but instead is used to encode any unknown word.\n",
    "\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "Complete the lab exercises for this week before following these steps to complete your assignment.\n",
    "\n",
    "Using the Keras dataset, create a new notebook and perform each of the following data preparation tasks and answer the related questions:\n",
    "\n",
    "1. Read Reuters dataset into training and testing \n",
    "2. Prepare dataset\n",
    "3. Build and compile 3 different models using Keras LTSM ideally improving model at each iteration.\n",
    "4. Describe and explain your findings.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "friendly-brazilian",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loading Necessary Packages \n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accredited-subdivision",
   "metadata": {},
   "source": [
    "### 1. Read Reuters dataset into training and testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "acquired-fruit",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loading Reuters dataset \n",
    "## More information here: https://keras.io/api/datasets/reuters/\n",
    "\n",
    "data = tf.keras.datasets.reuters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "armed-insulin",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Splitting Data into Training and Testing datasets \n",
    "num_of_words=10000\n",
    "(x_train, y_train), (x_test, y_test) = data.load_data(num_words=num_of_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "atlantic-monroe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data (independent): \n",
      "[1, 2, 2, 8, 43, 10, 447, 5, 25, 207, 270, 5, 3095, 111, 16, 369, 186, 90, 67, 7, 89, 5, 19, 102, 6, 19, 124, 15, 90, 67, 84, 22, 482, 26, 7, 48, 4, 49, 8, 864, 39, 209, 154, 6, 151, 6, 83, 11, 15, 22, 155, 11, 15, 7, 48, 9, 4579, 1005, 504, 6, 258, 6, 272, 11, 15, 22, 134, 44, 11, 15, 16, 8, 197, 1245, 90, 67, 52, 29, 209, 30, 32, 132, 6, 109, 15, 17, 12]\n",
      "\n",
      "\n",
      "Training Data (dependent): \n",
      "[ 3  4  3 ... 25  3 25]\n"
     ]
    }
   ],
   "source": [
    "## Checking the data:\n",
    "print(\"Training Data (independent): \")\n",
    "print(x_train[0])\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Training Data (dependent): \")\n",
    "print(y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "agreed-burns",
   "metadata": {},
   "source": [
    "### 2. Prepare dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "numeric-insert",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A dictionary mapping words to an integer index\n",
    "word_index = tf.keras.datasets.reuters.get_word_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "social-reputation",
   "metadata": {},
   "outputs": [],
   "source": [
    "## The first indices are reserved. Adding three to all key values \n",
    "word_index = {k:(v+3) for k,v in word_index.items()}\n",
    "word_index[\"<PAD>\"] = 0\n",
    "word_index[\"<START>\"] = 1\n",
    "word_index[\"<UNK>\"] = 2  # unknown\n",
    "word_index[\"<UNUSED>\"] = 3\n",
    "\n",
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "\n",
    "## Function to join words into sentences \n",
    "def decode_review(text):\n",
    "    return ' '.join([reverse_word_index.get(i, '?') for i in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "moved-tuning",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<START> opec believes world oil prices should be set around a fixed average price of 18 dlrs a barrel <UNK> assistant general secretary <UNK> al wattari said today in a speech to a european community ec <UNK> opec seminar in luxembourg released here al wattari said opec believes the world energy trade should be kept without restrictions and should be built around a fixed average price of 18 dlrs but he warned that defense of the 18 dlr a barrel level had caused hardship for opec countries who had been forced to curtail production and he warned that such cutbacks by opec states could not be sustained in some cases for opec to stabilize the world oil price at what is now considered the optimal level of 18 dlrs a barrel its member countries have had to undergo severe hardship in <UNK> production al wattari said such cutbacks cannot in certain cases be sustained al wattari said as well as financial and marketing pressures some states depended on associated gas output for domestic use and oil cutbacks had left insufficient gas supplies he added al wattari noted that total opec output was below the organization's agreed ceiling for all member countries in february although this had meant sacrifices the effect of these sacrifices meant that market stability though restored to a good level was still under pressure al wattari said a lasting stability in the world market requires a wider scope of international cooperation he added he said some non opec oil producing countries had shown a political willingness after 1986 to cooperate with opec but although cutbacks announced by these states were politically significant and welcomed by opec they were insufficient in terms of volume he added the overall majority of non opec producers have not responded sufficiently to opec's calls for supply regulation he said al wattari said an 18 dlr a barrel price was optimal as it allowed investment in the oil industry outside opec to continue while not generating excessive cash flow for otherwise <UNK> high cost areas outside opec such a price would no longer encourage protectionist measures he added <UNK> al chalabi opec deputy secretary general also addressing the seminar added that discipline was still needed to prevent <UNK> fluctuations in the oil market cooperation between arab states and europe was advantageous for both sides al chalabi said adding he hoped cooperation would ultimately lead to full <UNK> <UNK> arab dialogue reuter 3\""
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Checking to see if the sentences appearing correctly \n",
    "decode_review(x_train[100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "equal-eating",
   "metadata": {},
   "source": [
    "### 3. Build and compile 3 different models using Keras LTSM ideally improving model at each iteration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "architectural-texture",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only consider the first 350 words within the review\n",
    "max_review_length = 350\n",
    "x_train = keras.preprocessing.sequence.pad_sequences(x_train, maxlen=max_review_length)\n",
    "x_test = keras.preprocessing.sequence.pad_sequences(x_test, maxlen=max_review_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ordinary-gothic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_19\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_18 (Embedding)     (None, 350, 25)           250000    \n",
      "_________________________________________________________________\n",
      "lstm_18 (LSTM)               (None, 100)               50400     \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 46)                4646      \n",
      "=================================================================\n",
      "Total params: 305,046\n",
      "Trainable params: 305,046\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/3\n",
      "180/180 [==============================] - 74s 394ms/step - loss: 3.6424 - accuracy: 0.3417 - val_loss: 2.7573 - val_accuracy: 0.3620\n",
      "Epoch 2/3\n",
      "180/180 [==============================] - 69s 383ms/step - loss: 2.6435 - accuracy: 0.3483 - val_loss: 2.4903 - val_accuracy: 0.3620\n",
      "Epoch 3/3\n",
      "180/180 [==============================] - 69s 383ms/step - loss: 2.4957 - accuracy: 0.3445 - val_loss: 2.4367 - val_accuracy: 0.3620\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9903ba3820>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Construct our model #1\n",
    "embedding_vecor_length = 25\n",
    "model1 = keras.models.Sequential()\n",
    "model1.add(keras.layers.Embedding(num_of_words, embedding_vecor_length, input_length=max_review_length))\n",
    "model1.add(keras.layers.LSTM(100))\n",
    "model1.add(keras.layers.Dense(46, activation='sigmoid'))\n",
    "model1.compile(loss='sparse_categorical_crossentropy', optimizer='SGD', metrics=['accuracy'])\n",
    "print(model1.summary())\n",
    "model1.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=3, batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "imperial-comment",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 36.20%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model #1\n",
    "scores = model1.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "tamil-postage",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_20\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_19 (Embedding)     (None, 350, 50)           500000    \n",
      "_________________________________________________________________\n",
      "lstm_19 (LSTM)               (None, 100)               60400     \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 46)                4646      \n",
      "=================================================================\n",
      "Total params: 565,046\n",
      "Trainable params: 565,046\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/3\n",
      "90/90 [==============================] - 66s 693ms/step - loss: 3.2185 - accuracy: 0.3312 - val_loss: 2.3988 - val_accuracy: 0.3620\n",
      "Epoch 2/3\n",
      "90/90 [==============================] - 62s 694ms/step - loss: 2.3810 - accuracy: 0.3496 - val_loss: 2.4089 - val_accuracy: 0.3620\n",
      "Epoch 3/3\n",
      "90/90 [==============================] - 62s 686ms/step - loss: 2.4289 - accuracy: 0.3501 - val_loss: 2.4013 - val_accuracy: 0.3620\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9904d40c70>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Construct our model #2\n",
    "embedding_vecor_length = 50\n",
    "model2 = keras.models.Sequential()\n",
    "model2.add(keras.layers.Embedding(num_of_words, embedding_vecor_length, input_length=max_review_length))\n",
    "model2.add(keras.layers.LSTM(100))\n",
    "model2.add(keras.layers.Dense(46, activation='sigmoid'))\n",
    "model2.compile(loss='sparse_categorical_crossentropy', optimizer='Adamax', metrics=['accuracy'])\n",
    "print(model2.summary())\n",
    "model2.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=3, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "shaped-electric",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 36.20%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model #2\n",
    "scores = model2.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "physical-haiti",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_21\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_20 (Embedding)     (None, 350, 32)           320000    \n",
      "_________________________________________________________________\n",
      "lstm_20 (LSTM)               (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 46)                4646      \n",
      "=================================================================\n",
      "Total params: 377,846\n",
      "Trainable params: 377,846\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/3\n",
      "141/141 [==============================] - 65s 448ms/step - loss: 2.8628 - accuracy: 0.3304 - val_loss: 2.3478 - val_accuracy: 0.3620\n",
      "Epoch 2/3\n",
      "141/141 [==============================] - 65s 459ms/step - loss: 2.1573 - accuracy: 0.4615 - val_loss: 2.1400 - val_accuracy: 0.4715\n",
      "Epoch 3/3\n",
      "141/141 [==============================] - 90s 638ms/step - loss: 1.9986 - accuracy: 0.4979 - val_loss: 1.8773 - val_accuracy: 0.4960\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9923c246d0>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Construct our model #3\n",
    "embedding_vecor_length = 32\n",
    "model3 = keras.models.Sequential()\n",
    "model3.add(keras.layers.Embedding(num_of_words, embedding_vecor_length, input_length=max_review_length))\n",
    "model3.add(keras.layers.LSTM(100))\n",
    "model3.add(keras.layers.Dense(46, activation='sigmoid'))\n",
    "model3.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model3.summary())\n",
    "model3.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=3, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "annual-cache",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 49.60%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model #3\n",
    "scores = model3.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "competent-tract",
   "metadata": {},
   "source": [
    "### 4. Describe and explain your findings.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "public-restriction",
   "metadata": {},
   "source": [
    "Above I tested three different RNN models that processed and categorized Reuters news data.  In the first model I used the sparse categorical cross entropy loss function, the SGD optimizer, 100 size for the LSTM layer, and 25 for the embedding vector length. The accuracy after the third epoch was 36.20%, and the testing accuracy was also 36.20%.\n",
    "\n",
    "In  the second model I kept the sparse categorical cross entropy loss function and the same LSTM layer, however changed the  embedding vector length to 50, updated the optimizer to Adamax, and the batch_size to 100. Interestingly enough, this gave me the same accuracy of 36.20% above for testing and training. \n",
    "\n",
    "In the third model, I once again kept the sparse categorical cross entropy loss function and the same LSTM layer. I altered the embedding vector layer to 32, the optimized to adam, and the batch size to 64. This resulted in a higher accuracy score of 49.6% after the third epoch and the testing accuracy was also 49.60%.\n",
    "\n",
    "What I found when testing different hyperparameters was that the categorical cross entropy loss function performed the best - which is why I did not change it across models. Instead, I altered the optimizer, embedding vector length, and batch size. Since the highest accuracy score I obtained was still low (at 49.6%) I would continue adjusting the hyper parameters to increase this. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exceptional-interface",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
