{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "patient-simon",
   "metadata": {},
   "source": [
    "## Lesson 4: Predictive Policing\n",
    "### Author: Ana Javed\n",
    "\n",
    "#### Workplace Scenario\n",
    "\n",
    "You are working for a data science consulting company. Your company is approached by a client requesting that you analyze crime data across the United States. At first glance, you notice that the data has 128 attributes and cannot be examined manually. The data combines socio-economic data from the 1990 US Census, law enforcement data from the 1990 US LEMAS survey, and crime data from the 1995 FBI UCR. You are tasked to identify which are the most important features or attributes that contribute to crime. \n",
    "\n",
    "Generally, such data might be used for predictive policing, where police departments can predict potential criminal activity so they can ensure they are properly staffed and the areas of concern are patrolled accordingly.\n",
    "\n",
    "##### Instructions\n",
    "\n",
    "It is recommended you complete the lab exercises for this lesson before beginning the assignment.\n",
    "\n",
    "Using the Communities and Crime dataset (http://archive.ics.uci.edu/ml/machine-learning-databases/communities/), create a new notebook and perform each of the following tasks and answer the related questions:\n",
    "\n",
    "    - Read data.\n",
    "    - Apply three techniques for filter selection: Filter methods, Wrapper methods, Embedded methods.\n",
    "    - Describe your findings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "copyrighted-arlington",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importing Necessary Libraries & Packages \n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import csv\n",
    "import sklearn \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "gentle-wagner",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reading data file into Dataframe \n",
    "col_name_list = [\"state\",\"county\",\"community\",\"communityname\",\"fold\",\"population\",\n",
    "\"householdsize\",\"racepctblack\",\"racePctWhite\",\"racePctAsian\",\"racePctHisp\",\"agePct12t21\",\n",
    "\"agePct12t29\",\"agePct16t24\",\"agePct65up\",\"numbUrban\",\"pctUrban\",\"medIncome\",\"pctWWage\",\n",
    "\"pctWFarmSelf\",\"pctWInvInc\",\"pctWSocSec\",\"pctWPubAsst\",\"pctWRetire\",\"medFamInc\",\"perCapInc\",\n",
    "\"whitePerCap\",\"blackPerCap\",\"indianPerCap\",\"AsianPerCap\",\"OtherPerCap\",\"HispPerCap\",\n",
    "\"NumUnderPov\",\"PctPopUnderPov\",\"PctLess9thGrade\",\"PctNotHSGrad\",\"PctBSorMore\",\n",
    "\"PctUnemployed\",\"PctEmploy\",\"PctEmplManu\",\"PctEmplProfServ\",\"PctOccupManu\",\n",
    "\"PctOccupMgmtProf\",\"MalePctDivorce\",\"MalePctNevMarr\",\"FemalePctDiv\",\"TotalPctDiv\",\n",
    "\"PersPerFam\",\"PctFam2Par\",\"PctKids2Par\",\"PctYoungKids2Par\",\"PctTeen2Par\",\n",
    "\"PctWorkMomYoungKids\",\"PctWorkMom\",\"NumIlleg\",\"PctIlleg\",\"NumImmig\",\"PctImmigRecent\",\n",
    "\"PctImmigRec5\",\"PctImmigRec8\",\"PctImmigRec10\",\"PctRecentImmig\",\"PctRecImmig5\",\n",
    "\"PctRecImmig8\",\"PctRecImmig10\",\"PctSpeakEnglOnly\",\"PctNotSpeakEnglWell\",\"PctLargHouseFam\",\n",
    "\"PctLargHouseOccup\",\"PersPerOccupHous\",\"PersPerOwnOccHous\",\"PersPerRentOccHous\",\"PctPersOwnOccup\",\n",
    "\"PctPersDenseHous\",\"PctHousLess3BR\",\"MedNumBR\",\"HousVacant\",\"PctHousOccup\",\n",
    "\"PctHousOwnOcc\",\"PctVacantBoarded\",\"PctVacMore6Mos\",\"MedYrHousBuilt\",\"PctHousNoPhone\",\n",
    "\"PctWOFullPlumb\",\"OwnOccLowQuart\",\"OwnOccMedVal\",\"OwnOccHiQuart\",\"RentLowQ\",\n",
    "\"RentMedian\",\"RentHighQ\",\"MedRent\",\"MedRentPctHousInc\",\"MedOwnCostPctInc\",\"MedOwnCostPctIncNoMtg\",\n",
    "\"NumInShelters\",\"NumStreet\",\"PctForeignBorn\",\"PctBornSameState\",\"PctSameHouse85\",\n",
    "\"PctSameCity85\",\"PctSameState85\",\"LemasSwornFT\",\"LemasSwFTPerPop\",\"LemasSwFTFieldOps\",\n",
    "\"LemasSwFTFieldPerPop\",\"LemasTotalReq\",\"LemasTotReqPerPop\",\"PolicReqPerOffic\",\n",
    "\"PolicPerPop\",\"RacialMatchCommPol\",\"PctPolicWhite\",\"PctPolicBlack\",\"PctPolicHisp\",\n",
    "\"PctPolicAsian\",\"PctPolicMinor\",\"OfficAssgnDrugUnits\",\"NumKindsDrugsSeiz\",\"PolicAveOTWorked\",\n",
    "\"LandArea\",\"PopDens\",\"PctUsePubTrans\",\"PolicCars\",\"PolicOperBudg\",\"LemasPctPolicOnPatr\",\n",
    "\"LemasGangUnitDeploy\",\"LemasPctOfficDrugUn\",\"PolicBudgPerPop\",\"ViolentCrimesPerPop\"]\n",
    "\n",
    "url = \"http://archive.ics.uci.edu/ml/machine-learning-databases/communities/communities.data\"\n",
    "df = pd.read_csv(url, sep=\",\", names = col_name_list)\n",
    "\n",
    "## First & Last 5 Rows from Dataframe\n",
    "# print(df.head())\n",
    "# print(df.tail()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "enormous-muscle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1994, 128)\n",
      "state                    int64\n",
      "county                  object\n",
      "community               object\n",
      "communityname           object\n",
      "fold                     int64\n",
      "                        ...   \n",
      "LemasPctPolicOnPatr     object\n",
      "LemasGangUnitDeploy     object\n",
      "LemasPctOfficDrugUn    float64\n",
      "PolicBudgPerPop         object\n",
      "ViolentCrimesPerPop    float64\n",
      "Length: 128, dtype: object\n",
      "             state         fold   population  householdsize  racepctblack  \\\n",
      "count  1994.000000  1994.000000  1994.000000    1994.000000   1994.000000   \n",
      "mean     28.683551     5.493982     0.057593       0.463395      0.179629   \n",
      "std      16.397553     2.873694     0.126906       0.163717      0.253442   \n",
      "min       1.000000     1.000000     0.000000       0.000000      0.000000   \n",
      "25%      12.000000     3.000000     0.010000       0.350000      0.020000   \n",
      "50%      34.000000     5.000000     0.020000       0.440000      0.060000   \n",
      "75%      42.000000     8.000000     0.050000       0.540000      0.230000   \n",
      "max      56.000000    10.000000     1.000000       1.000000      1.000000   \n",
      "\n",
      "       racePctWhite  racePctAsian  racePctHisp  agePct12t21  agePct12t29  ...  \\\n",
      "count   1994.000000   1994.000000  1994.000000  1994.000000  1994.000000  ...   \n",
      "mean       0.753716      0.153681     0.144022     0.424218     0.493867  ...   \n",
      "std        0.244039      0.208877     0.232492     0.155196     0.143564  ...   \n",
      "min        0.000000      0.000000     0.000000     0.000000     0.000000  ...   \n",
      "25%        0.630000      0.040000     0.010000     0.340000     0.410000  ...   \n",
      "50%        0.850000      0.070000     0.040000     0.400000     0.480000  ...   \n",
      "75%        0.940000      0.170000     0.160000     0.470000     0.540000  ...   \n",
      "max        1.000000      1.000000     1.000000     1.000000     1.000000  ...   \n",
      "\n",
      "       PctForeignBorn  PctBornSameState  PctSameHouse85  PctSameCity85  \\\n",
      "count     1994.000000       1994.000000     1994.000000    1994.000000   \n",
      "mean         0.215552          0.608892        0.535050       0.626424   \n",
      "std          0.231134          0.204329        0.181352       0.200521   \n",
      "min          0.000000          0.000000        0.000000       0.000000   \n",
      "25%          0.060000          0.470000        0.420000       0.520000   \n",
      "50%          0.130000          0.630000        0.540000       0.670000   \n",
      "75%          0.280000          0.777500        0.660000       0.770000   \n",
      "max          1.000000          1.000000        1.000000       1.000000   \n",
      "\n",
      "       PctSameState85     LandArea      PopDens  PctUsePubTrans  \\\n",
      "count     1994.000000  1994.000000  1994.000000     1994.000000   \n",
      "mean         0.651530     0.065231     0.232854        0.161685   \n",
      "std          0.198221     0.109459     0.203092        0.229055   \n",
      "min          0.000000     0.000000     0.000000        0.000000   \n",
      "25%          0.560000     0.020000     0.100000        0.020000   \n",
      "50%          0.700000     0.040000     0.170000        0.070000   \n",
      "75%          0.790000     0.070000     0.280000        0.190000   \n",
      "max          1.000000     1.000000     1.000000        1.000000   \n",
      "\n",
      "       LemasPctOfficDrugUn  ViolentCrimesPerPop  \n",
      "count          1994.000000          1994.000000  \n",
      "mean              0.094052             0.237979  \n",
      "std               0.240328             0.232985  \n",
      "min               0.000000             0.000000  \n",
      "25%               0.000000             0.070000  \n",
      "50%               0.000000             0.150000  \n",
      "75%               0.000000             0.330000  \n",
      "max               1.000000             1.000000  \n",
      "\n",
      "[8 rows x 102 columns]\n"
     ]
    }
   ],
   "source": [
    "## Conducting Exploratory Data Analysis: \n",
    "    # Number of Instances: 1994\n",
    "    # Number of Attributes: 128\n",
    "    # Missing Values? Yes\n",
    "    # Data Set Characteristics:  Multivariate\n",
    "    # Attribute Characteristics: Real\n",
    "\n",
    "print(df.shape)  # (1993, 128)\n",
    "print(df.dtypes) \n",
    "print(df.describe()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "eastern-intervention",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to Find which column has the most missing values\n",
    "\n",
    "def missing_value_count(df):\n",
    "    missing_dict = {}\n",
    "\n",
    "    for i, row in enumerate(df.values):\n",
    "        if row[0] in col_name_list:\n",
    "            continue\n",
    "\n",
    "        # print(i, row)\n",
    "        for num, val in enumerate(row):\n",
    "            # print(num, val)\n",
    "            if val == '?':\n",
    "                if str(num) not in missing_dict.keys():\n",
    "                    missing_dict[str(num)] = 0\n",
    "                missing_dict[str(num)] += 1 \n",
    "            else:\n",
    "                continue\n",
    "                \n",
    "    return missing_dict \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "conditional-hello",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1': 1174, '2': 1177, '101': 1675, '102': 1675, '103': 1675, '104': 1675, '105': 1675, '106': 1675, '107': 1675, '108': 1675, '109': 1675, '110': 1675, '111': 1675, '112': 1675, '113': 1675, '114': 1675, '115': 1675, '116': 1675, '117': 1675, '121': 1675, '122': 1675, '123': 1675, '124': 1675, '126': 1675, '30': 1}\n",
      "Number of Columns Deleted:  22\n"
     ]
    }
   ],
   "source": [
    "## Missing Value Counts by Columns: \n",
    "missing_dict = missing_value_count(df)\n",
    "print(missing_dict)\n",
    "\n",
    "\"\"\"\n",
    "It appears columns 101 - 117, 121-124, and 126 have the most missing values (1675 rows each). \n",
    "I will drop these columns since more than half of the values are missing. \n",
    "For other columns that have fewer missing values, I will impute them \n",
    "with the column median values if they are numeric. \n",
    "\"\"\"\n",
    "\n",
    "# print(missing_dict.keys())\n",
    "\n",
    "\n",
    "# Columns to Drop: 101 - 117, 121-124, 126\n",
    "counter = 0 \n",
    "for i, name in enumerate(col_name_list):\n",
    "    if (i in range(101, 118, 1)) or (i in range(121, 125, 1))  or (i == 126):\n",
    "        # print(name)\n",
    "        df = df.drop(columns=name)\n",
    "        del missing_dict[str(i)]\n",
    "        counter +=1\n",
    "\n",
    "print(\"Number of Columns Deleted: \", counter)\n",
    "\n",
    "\n",
    "## Also Deleting the One String Column: communityname \n",
    "df = df.drop(columns=  'communityname')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "possible-catch",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with Remaining Missing Values: \n",
      "{'1': 1174, '2': 1177, '29': 1}\n"
     ]
    }
   ],
   "source": [
    "### Imputing the Missing Values with the Median value (if applicable) \n",
    "print(\"Columns with Remaining Missing Values: \")\n",
    "missing_dict = missing_value_count(df)\n",
    "print(missing_dict)\n",
    "# Remaining columns are: county, community, and OtherPerCap\n",
    "\n",
    "\n",
    "df.loc[:, \"county\"] = pd.to_numeric(df.loc[:, \"county\"], errors='coerce')\n",
    "HasNan1 = np.isnan(df.loc[:, \"county\"] )\n",
    "# sum(HasNan1)  # 1174\n",
    "df.loc[HasNan1, \"county\"] = np.nanmedian(df.loc[:, \"county\"] )\n",
    "\n",
    "\n",
    "df.loc[:, \"community\"] = pd.to_numeric(df.loc[:, \"community\"], errors='coerce')\n",
    "HasNan1 = np.isnan(df.loc[:, \"community\"] )\n",
    "# sum(HasNan1)  # 1177\n",
    "df.loc[HasNan1, \"community\"] = np.nanmedian(df.loc[:, \"community\"] )\n",
    "\n",
    "\n",
    "df.loc[:, \"OtherPerCap\"] = pd.to_numeric(df.loc[:, \"OtherPerCap\"], errors='coerce')\n",
    "HasNan1 = np.isnan(df.loc[:, \"OtherPerCap\"] )\n",
    "# sum(HasNan1)  # 1\n",
    "df.loc[HasNan1, \"OtherPerCap\"] = np.nanmedian(df.loc[:, \"OtherPerCap\"] )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "numerous-requirement",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with Remaining Missing Values: \n",
      "{}\n",
      "\n",
      "Dataframe Dimensions: \n",
      "(1994, 105)\n"
     ]
    }
   ],
   "source": [
    "## Checking Once More Regarding Missing values:\n",
    "print(\"Columns with Remaining Missing Values: \")\n",
    "missing_dict = missing_value_count(df)\n",
    "print(missing_dict) \n",
    "\n",
    "\n",
    "print(\"\\nDataframe Dimensions: \")\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reduced-pottery",
   "metadata": {},
   "source": [
    "#### Now Z-Normalizing the Dataset \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "enhanced-trademark",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "      <th>100</th>\n",
       "      <th>101</th>\n",
       "      <th>102</th>\n",
       "      <th>103</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.994000e+03</td>\n",
       "      <td>1.994000e+03</td>\n",
       "      <td>1.994000e+03</td>\n",
       "      <td>1.994000e+03</td>\n",
       "      <td>1.994000e+03</td>\n",
       "      <td>1.994000e+03</td>\n",
       "      <td>1.994000e+03</td>\n",
       "      <td>1.994000e+03</td>\n",
       "      <td>1.994000e+03</td>\n",
       "      <td>1.994000e+03</td>\n",
       "      <td>...</td>\n",
       "      <td>1.994000e+03</td>\n",
       "      <td>1.994000e+03</td>\n",
       "      <td>1.994000e+03</td>\n",
       "      <td>1.994000e+03</td>\n",
       "      <td>1.994000e+03</td>\n",
       "      <td>1.994000e+03</td>\n",
       "      <td>1.994000e+03</td>\n",
       "      <td>1.994000e+03</td>\n",
       "      <td>1.994000e+03</td>\n",
       "      <td>1.994000e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-9.443020e-17</td>\n",
       "      <td>2.138042e-17</td>\n",
       "      <td>-1.812882e-16</td>\n",
       "      <td>-5.701446e-17</td>\n",
       "      <td>4.187000e-17</td>\n",
       "      <td>-4.525523e-16</td>\n",
       "      <td>1.042296e-16</td>\n",
       "      <td>1.692617e-16</td>\n",
       "      <td>4.231542e-17</td>\n",
       "      <td>1.247191e-17</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.563404e-17</td>\n",
       "      <td>4.187000e-17</td>\n",
       "      <td>4.774961e-16</td>\n",
       "      <td>-1.781702e-17</td>\n",
       "      <td>2.494383e-17</td>\n",
       "      <td>2.512200e-16</td>\n",
       "      <td>4.632425e-17</td>\n",
       "      <td>5.256021e-17</td>\n",
       "      <td>7.928574e-17</td>\n",
       "      <td>-1.069021e-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.000251e+00</td>\n",
       "      <td>1.000251e+00</td>\n",
       "      <td>1.000251e+00</td>\n",
       "      <td>1.000251e+00</td>\n",
       "      <td>1.000251e+00</td>\n",
       "      <td>1.000251e+00</td>\n",
       "      <td>1.000251e+00</td>\n",
       "      <td>1.000251e+00</td>\n",
       "      <td>1.000251e+00</td>\n",
       "      <td>1.000251e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000251e+00</td>\n",
       "      <td>1.000251e+00</td>\n",
       "      <td>1.000251e+00</td>\n",
       "      <td>1.000251e+00</td>\n",
       "      <td>1.000251e+00</td>\n",
       "      <td>1.000251e+00</td>\n",
       "      <td>1.000251e+00</td>\n",
       "      <td>1.000251e+00</td>\n",
       "      <td>1.000251e+00</td>\n",
       "      <td>1.000251e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-1.688697e+00</td>\n",
       "      <td>-4.430138e-01</td>\n",
       "      <td>-2.914039e+00</td>\n",
       "      <td>-1.564227e+00</td>\n",
       "      <td>-4.539368e-01</td>\n",
       "      <td>-2.831179e+00</td>\n",
       "      <td>-7.089350e-01</td>\n",
       "      <td>-3.089277e+00</td>\n",
       "      <td>-7.359319e-01</td>\n",
       "      <td>-6.196275e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.269325e-01</td>\n",
       "      <td>-9.328163e-01</td>\n",
       "      <td>-2.980711e+00</td>\n",
       "      <td>-2.951081e+00</td>\n",
       "      <td>-3.124761e+00</td>\n",
       "      <td>-3.287706e+00</td>\n",
       "      <td>-5.960859e-01</td>\n",
       "      <td>-1.146831e+00</td>\n",
       "      <td>-7.060564e-01</td>\n",
       "      <td>-3.914469e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-1.017697e+00</td>\n",
       "      <td>-1.776870e-01</td>\n",
       "      <td>4.806272e-02</td>\n",
       "      <td>-8.680839e-01</td>\n",
       "      <td>-3.751184e-01</td>\n",
       "      <td>-6.928041e-01</td>\n",
       "      <td>-6.300017e-01</td>\n",
       "      <td>-5.070787e-01</td>\n",
       "      <td>-5.443840e-01</td>\n",
       "      <td>-5.766044e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.269325e-01</td>\n",
       "      <td>-6.731617e-01</td>\n",
       "      <td>-6.799173e-01</td>\n",
       "      <td>-6.345617e-01</td>\n",
       "      <td>-5.308709e-01</td>\n",
       "      <td>-4.618707e-01</td>\n",
       "      <td>-4.133235e-01</td>\n",
       "      <td>-6.543192e-01</td>\n",
       "      <td>-6.187192e-01</td>\n",
       "      <td>-3.914469e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.243035e-01</td>\n",
       "      <td>-1.776870e-01</td>\n",
       "      <td>4.806272e-02</td>\n",
       "      <td>-1.719410e-01</td>\n",
       "      <td>-2.963001e-01</td>\n",
       "      <td>-1.429362e-01</td>\n",
       "      <td>-4.721351e-01</td>\n",
       "      <td>3.946412e-01</td>\n",
       "      <td>-4.007231e-01</td>\n",
       "      <td>-4.475351e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.269325e-01</td>\n",
       "      <td>-3.702314e-01</td>\n",
       "      <td>1.033317e-01</td>\n",
       "      <td>2.730101e-02</td>\n",
       "      <td>2.173666e-01</td>\n",
       "      <td>2.445882e-01</td>\n",
       "      <td>-2.305611e-01</td>\n",
       "      <td>-3.095611e-01</td>\n",
       "      <td>-4.003760e-01</td>\n",
       "      <td>-3.914469e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>8.123035e-01</td>\n",
       "      <td>-1.776870e-01</td>\n",
       "      <td>4.806272e-02</td>\n",
       "      <td>8.722733e-01</td>\n",
       "      <td>-5.984503e-02</td>\n",
       "      <td>4.680281e-01</td>\n",
       "      <td>1.987979e-01</td>\n",
       "      <td>7.635267e-01</td>\n",
       "      <td>7.814654e-02</td>\n",
       "      <td>6.874201e-02</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.269325e-01</td>\n",
       "      <td>2.789052e-01</td>\n",
       "      <td>8.253895e-01</td>\n",
       "      <td>6.891637e-01</td>\n",
       "      <td>7.161916e-01</td>\n",
       "      <td>6.987404e-01</td>\n",
       "      <td>4.358251e-02</td>\n",
       "      <td>2.322017e-01</td>\n",
       "      <td>1.236475e-01</td>\n",
       "      <td>-3.914469e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.666304e+00</td>\n",
       "      <td>9.675587e+00</td>\n",
       "      <td>2.916835e+00</td>\n",
       "      <td>1.568416e+00</td>\n",
       "      <td>7.427898e+00</td>\n",
       "      <td>3.278464e+00</td>\n",
       "      <td>3.237730e+00</td>\n",
       "      <td>1.009450e+00</td>\n",
       "      <td>4.052765e+00</td>\n",
       "      <td>3.682682e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>9.735713e+00</td>\n",
       "      <td>3.394760e+00</td>\n",
       "      <td>1.914595e+00</td>\n",
       "      <td>2.564441e+00</td>\n",
       "      <td>1.863489e+00</td>\n",
       "      <td>1.758429e+00</td>\n",
       "      <td>8.542034e+00</td>\n",
       "      <td>3.778285e+00</td>\n",
       "      <td>3.660806e+00</td>\n",
       "      <td>3.770572e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 104 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                0             1             2             3             4    \\\n",
       "count  1.994000e+03  1.994000e+03  1.994000e+03  1.994000e+03  1.994000e+03   \n",
       "mean  -9.443020e-17  2.138042e-17 -1.812882e-16 -5.701446e-17  4.187000e-17   \n",
       "std    1.000251e+00  1.000251e+00  1.000251e+00  1.000251e+00  1.000251e+00   \n",
       "min   -1.688697e+00 -4.430138e-01 -2.914039e+00 -1.564227e+00 -4.539368e-01   \n",
       "25%   -1.017697e+00 -1.776870e-01  4.806272e-02 -8.680839e-01 -3.751184e-01   \n",
       "50%    3.243035e-01 -1.776870e-01  4.806272e-02 -1.719410e-01 -2.963001e-01   \n",
       "75%    8.123035e-01 -1.776870e-01  4.806272e-02  8.722733e-01 -5.984503e-02   \n",
       "max    1.666304e+00  9.675587e+00  2.916835e+00  1.568416e+00  7.427898e+00   \n",
       "\n",
       "                5             6             7             8             9    \\\n",
       "count  1.994000e+03  1.994000e+03  1.994000e+03  1.994000e+03  1.994000e+03   \n",
       "mean  -4.525523e-16  1.042296e-16  1.692617e-16  4.231542e-17  1.247191e-17   \n",
       "std    1.000251e+00  1.000251e+00  1.000251e+00  1.000251e+00  1.000251e+00   \n",
       "min   -2.831179e+00 -7.089350e-01 -3.089277e+00 -7.359319e-01 -6.196275e-01   \n",
       "25%   -6.928041e-01 -6.300017e-01 -5.070787e-01 -5.443840e-01 -5.766044e-01   \n",
       "50%   -1.429362e-01 -4.721351e-01  3.946412e-01 -4.007231e-01 -4.475351e-01   \n",
       "75%    4.680281e-01  1.987979e-01  7.635267e-01  7.814654e-02  6.874201e-02   \n",
       "max    3.278464e+00  3.237730e+00  1.009450e+00  4.052765e+00  3.682682e+00   \n",
       "\n",
       "       ...           94            95            96            97   \\\n",
       "count  ...  1.994000e+03  1.994000e+03  1.994000e+03  1.994000e+03   \n",
       "mean   ... -3.563404e-17  4.187000e-17  4.774961e-16 -1.781702e-17   \n",
       "std    ...  1.000251e+00  1.000251e+00  1.000251e+00  1.000251e+00   \n",
       "min    ... -2.269325e-01 -9.328163e-01 -2.980711e+00 -2.951081e+00   \n",
       "25%    ... -2.269325e-01 -6.731617e-01 -6.799173e-01 -6.345617e-01   \n",
       "50%    ... -2.269325e-01 -3.702314e-01  1.033317e-01  2.730101e-02   \n",
       "75%    ... -2.269325e-01  2.789052e-01  8.253895e-01  6.891637e-01   \n",
       "max    ...  9.735713e+00  3.394760e+00  1.914595e+00  2.564441e+00   \n",
       "\n",
       "                98            99            100           101           102  \\\n",
       "count  1.994000e+03  1.994000e+03  1.994000e+03  1.994000e+03  1.994000e+03   \n",
       "mean   2.494383e-17  2.512200e-16  4.632425e-17  5.256021e-17  7.928574e-17   \n",
       "std    1.000251e+00  1.000251e+00  1.000251e+00  1.000251e+00  1.000251e+00   \n",
       "min   -3.124761e+00 -3.287706e+00 -5.960859e-01 -1.146831e+00 -7.060564e-01   \n",
       "25%   -5.308709e-01 -4.618707e-01 -4.133235e-01 -6.543192e-01 -6.187192e-01   \n",
       "50%    2.173666e-01  2.445882e-01 -2.305611e-01 -3.095611e-01 -4.003760e-01   \n",
       "75%    7.161916e-01  6.987404e-01  4.358251e-02  2.322017e-01  1.236475e-01   \n",
       "max    1.863489e+00  1.758429e+00  8.542034e+00  3.778285e+00  3.660806e+00   \n",
       "\n",
       "                103  \n",
       "count  1.994000e+03  \n",
       "mean  -1.069021e-17  \n",
       "std    1.000251e+00  \n",
       "min   -3.914469e-01  \n",
       "25%   -3.914469e-01  \n",
       "50%   -3.914469e-01  \n",
       "75%   -3.914469e-01  \n",
       "max    3.770572e+00  \n",
       "\n",
       "[8 rows x 104 columns]"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Z-Normalizing the attributes: \n",
    "X = df.loc[:, list(df.columns[:104])]\n",
    "y = df.loc[:, \"ViolentCrimesPerPop\"]\n",
    "# print(X.head())\n",
    "\n",
    "standardization_scale = StandardScaler().fit(X)\n",
    "X = standardization_scale.transform(X)\n",
    "X = pd.DataFrame(X) \n",
    "\n",
    "X.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sustainable-sampling",
   "metadata": {},
   "source": [
    "#### Now that I've removed all missing values from the data, standardized values, and trimmed the columns down to 106 - next is to apply feature selection methods\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aquatic-investigator",
   "metadata": {},
   "source": [
    "### First Method: Filter methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "smooth-lying",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "racepctblack .... Column: 6 .... 0.6312636346597023\n",
      "racePctWhite .... Column: 7 .... -0.6847695762715443\n",
      "PctFam2Par .... Column: 47 .... -0.7066674691569855\n",
      "PctKids2Par .... Column: 48 .... -0.7384238020704434\n",
      "PctYoungKids2Par .... Column: 49 .... -0.6660588959347982\n",
      "PctTeen2Par .... Column: 50 .... -0.6615816444304072\n",
      "PctIlleg .... Column: 54 .... 0.7379565498586647\n",
      "Total:  7\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.metrics import mutual_info_score\n",
    "## Conducting Pair-wise correlation for Columns in The Dataframe\n",
    "\n",
    "\n",
    "def correlation_above_threshold(df, threshold=0.5):\n",
    "    at_or_above_threshold = {}\n",
    "    \n",
    "    for i, val in enumerate(corr_df['ViolentCrimesPerPop']):\n",
    "        if abs(val) >= threshold:\n",
    "            if df.columns[i] != 'ViolentCrimesPerPop':\n",
    "                print (df.columns[i], \".... Column:\", i, \"....\", val)\n",
    "                at_or_above_threshold[df.columns[i]] = val\n",
    "                \n",
    "    return at_or_above_threshold \n",
    "\n",
    "\n",
    "## Creating a Pair-Wise Correlation Df \n",
    "corr_df = df.corr('pearson')\n",
    "# corr_df.head(15)\n",
    "\n",
    "## Finding Correlations Above Specified Threshold (using function above)\n",
    "output = correlation_above_threshold(corr_df, threshold = 0.6)\n",
    "print(\"Total: \", len(output)) \n",
    "\n",
    "# when threshold = 0.5, 15 variables; when threshold =0.6, 7 variables\n",
    "# when threshold = 0.7, 3 variables; when threshold = 0.8, 0 variables \n",
    "# output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distributed-adjustment",
   "metadata": {},
   "source": [
    "### Second Method: Wrapper methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "embedded-program",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anajaved/opt/miniconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass n_features_to_select=7 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  warnings.warn(f\"Pass {args_msg} as keyword args. From version \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False False False False False False False False False False False False\n",
      " False False  True False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False  True False  True  True False False\n",
      "  True False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False  True\n",
      "  True False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "[49 36 85 79  2 91  3 55 84 31 39 25 42 76  1 23 17 21 53 24 41 97 38 20\n",
      "  5  6 78 77 82 50 71 59 33 51 52 70 90 22 47 87 48 34  1 26  1  1 40 93\n",
      "  1 63 98 58 29 44 12 35 72 81 74 75 73 10  9 57 92 19 66 28 16 37 15 14\n",
      "  4 64 65 11 60 13 45 46 80 68 86  1  1 54  8 96 30  7 69 61 27 43 32 18\n",
      " 67 94 95 88 83 89 56 62]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_friedman1\n",
    "from sklearn.feature_selection import RFE #Recursive Feature Elimination\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "estimator = LinearRegression()\n",
    "selector = RFE(estimator, 7, step=1) #Step=1 means each step only remove 1 variable from the model\n",
    "selector = selector.fit(X, y)\n",
    "print(selector.support_) # The mask of selected features.\n",
    "print(selector.ranking_) # selected features are ranked 1. The 6th is the one that is removed first,\n",
    "                         # 2nd is the one that is removed last\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "tutorial-commissioner",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numbUrban .... Column: 14 .... 1\n",
      "MalePctDivorce .... Column: 42 .... 1\n",
      "FemalePctDiv .... Column: 44 .... 1\n",
      "TotalPctDiv .... Column: 45 .... 1\n",
      "PctKids2Par .... Column: 48 .... 1\n",
      "OwnOccLowQuart .... Column: 83 .... 1\n",
      "OwnOccMedVal .... Column: 84 .... 1\n"
     ]
    }
   ],
   "source": [
    "for i, val in enumerate(selector.ranking_):\n",
    "    # print(i, val)\n",
    "    if str(val) == '1':\n",
    "        if df.columns[i] != 'ViolentCrimesPerPop':\n",
    "            print (df.columns[i], \".... Column:\", i, \"....\", val)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bearing-electronics",
   "metadata": {},
   "source": [
    "### Third Method: Embedded methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "british-request",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.         -0.         -0.         -0.          0.         -0.\n",
      "  0.         -0.04264457  0.          0.         -0.         -0.\n",
      " -0.          0.          0.          0.         -0.         -0.\n",
      " -0.         -0.          0.          0.         -0.         -0.\n",
      " -0.         -0.         -0.         -0.          0.          0.\n",
      " -0.          0.          0.          0.          0.         -0.\n",
      "  0.         -0.         -0.         -0.          0.         -0.\n",
      "  0.0082995   0.          0.          0.00024754  0.         -0.\n",
      " -0.06535678 -0.         -0.         -0.         -0.          0.\n",
      "  0.04202239  0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.         -0.          0.\n",
      "  0.          0.          0.         -0.          0.         -0.\n",
      "  0.00514212  0.         -0.          0.01755062 -0.         -0.\n",
      "  0.         -0.          0.          0.          0.          0.\n",
      "  0.          0.         -0.          0.          0.          0.\n",
      "  0.          0.         -0.          0.          0.00440886  0.\n",
      " -0.         -0.          0.         -0.          0.          0.\n",
      "  0.          0.        ]\n",
      "0.23797893681043128\n"
     ]
    }
   ],
   "source": [
    "# LASSO Embedded Method \n",
    "from sklearn import linear_model\n",
    "\n",
    "alpha = 0.025 # Increasing alpha can shrink variable coefficients more to 0\n",
    "clf = linear_model.Lasso(alpha=alpha)\n",
    "clf.fit(X, y)\n",
    "\n",
    "print(clf.coef_)\n",
    "print(clf.intercept_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "elementary-mills",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "racePctWhite .... Column: 7 .... -0.04264457357393647\n",
      "MalePctDivorce .... Column: 42 .... 0.008299504974689956\n",
      "TotalPctDiv .... Column: 45 .... 0.0002475415149962874\n",
      "PctKids2Par .... Column: 48 .... -0.06535677567041379\n",
      "PctIlleg .... Column: 54 .... 0.04202239072487789\n",
      "PctPersDenseHous .... Column: 72 .... 0.005142119558077064\n",
      "HousVacant .... Column: 75 .... 0.017550618444137436\n",
      "NumStreet .... Column: 94 .... 0.004408861637625294\n"
     ]
    }
   ],
   "source": [
    "for i, val in enumerate(clf.coef_):\n",
    "    # print(i, val)\n",
    "    if str(abs(val)) != '0.0':\n",
    "        if df.columns[i] != 'ViolentCrimesPerPop':\n",
    "            print (df.columns[i], \".... Column:\", i, \"....\", val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "urban-dependence",
   "metadata": {},
   "source": [
    "### Summary \n",
    "\n",
    "The policing dataset explored in this assignment originally contained 128 attributes and had missing data values, which necessitated the use of different feature selection methods after cleaning and standardizing the data. \n",
    "\n",
    "The first feature selection method applied was the Filter method. I conducted pairwise correlation between the features and target variable, and only selected features that had a correlation above a specific threshold. I tested a few different threholds to see the number of attributes that returned. When I set the correlation threshold to 0.8 or above, no features returned. When I set the threshold to 0.6, then 7 attributes returned. These 7 attributes included: \n",
    "- racepctblack = percentage of population that is african american  (correlation: 0.631)\n",
    "- racePctWhite = percentage of population that is caucasian (correlation: -0.68)\n",
    "- PctFam2Par =  percentage of families (with kids) that are headed by two parents (correlation: -0.706)\n",
    "- PctKids2Par = percentage of kids in family housing with two parents (correlation: -0.738)\n",
    "- PctYoungKids2Par = percent of kids 4 and under in two parent households (correlation: -0.66)\n",
    "- PctTeen2Par = percent of kids age 12-17 in two parent households (correlation: -0.66)\n",
    "- PctIlleg= percentage of kids born to never married (correlation: 0.737)\n",
    "\n",
    "This list of attributes show that race and family dynamic (e.g. families headed by two parents) were the attributes with the most correlation to the total number of violent crimes per 100K popuation (aka the target variable). The variable that had the highest correlation (negative) was PctKids2Par, or the percentage of kids in housing with two parents. The next highest correlation (positive) was PctIlleg, or percentage of kids born to never married parents. While these two attributes are related to each other, it shows that family household is an important attribute.\n",
    "\n",
    "The second feature selection method applied was the Wrapper method. I specifically used the Backwards Step-wise feature selection with Recursive Feature Elimination. Since the first method resulted in 7 attributes, I passed 7 to the RFE() function to see how closely the attributes would match the filter method. The 7 attributes that returned included:\n",
    "- numbUrban = number of people living in areas classified as urban\n",
    "- MalePctDivorce = percentage of males who are divorced\n",
    "- FemalePctDiv =  percentage of females who are divorced\n",
    "- TotalPctDiv = percentage of population who are divorced\n",
    "- PctKids2Par = percentage of kids in family housing with two parents\n",
    "- OwnOccLowQuart = owner occupied housing - lower quartile value\n",
    "- OwnOccMedVal= owner occupied housing - median value\n",
    "\n",
    "This was interesting since this output only shows one attribute that the filter method showed (PctKids2Par), and we also see new attributes that may be related to crime - such as number owner occupied homes, divorced households, and community types. \n",
    "\n",
    "Lastly, the final feature selection method applied was the Embedded Method, specifically the LASSO method. I wanted to have the Lasso method provided around 7 attributes if possible (to easily compare between the different methods), and this ultimately depended on the alpha I passed to the function. A higher alpha value means a higher penalty for coefficients, and the more the coefficients will be closer to 0. With this information, I set the alpha = 0.025, and 8 attributes returned. They were: \n",
    "- racePctWhite = percentage of population that is caucasian  (coef = -0.0426)\n",
    "- MalePctDivorce = percentage of males who are divorced (coef = 0.008)\n",
    "- TotalPctDiv = percentage of population who are divorced  (coef = 0.0002)\n",
    "- PctKids2Par = percentage of kids in family housing with two parents (coef =  -0.065)\n",
    "- PctIlleg =  percentage of kids born to never married  (coef = 0.042)\n",
    "- PctPersDenseHous = percent of persons in dense housing (coef = 0.005)\n",
    "- HousVacant = number of vacant households (coef = 0.017)\n",
    "- NumStreet  = number of homeless people counted in the street (coef = 0.0044) \n",
    "\n",
    "Here we see race, divorced status, parent presence, and community statistics shown. Specifically we see new attribute such as the number of homes that are vacant, and number of homeless people in the street. The attribute with the highest coefficient was PctKids2Par (percentage of kids in family with two parents), and the next highest ones (respectively) were racePctWhite and PctIlleg. \n",
    "\n",
    "While there are some similarities in attributes across the methods, there were still new features that were introduced by each method. Now that we have different features selected, these features should be passed into a machine learning model to see which features best predict the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "higher-departure",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
